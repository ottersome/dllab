{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plt_ticker\n",
    "\n",
    "outfolder = \"pngs_V7\"\n",
    "if not os.path.exists(outfolder):\n",
    "    os.makedirs(outfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helpful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x sampled on sphere: [-0.49608509 -0.1678779  -0.48350907 -0.09528933  0.28424915 -0.30824554\n",
      " -0.02000082 -0.36425549 -0.36012506 -0.21040549]\n",
      "x@x: 1.0\n",
      "norm(x): 1.0\n",
      "\n",
      "Relu([-1,0,1]): [0 0 1]\n",
      "\n",
      "Differnent values of d & x@x: (x random normal)\n",
      "2 1.316181579068357\n",
      "5 10.64707663107968\n",
      "10 8.05986921727821\n",
      "20 22.007554560672897\n",
      "50 69.89716990462088\n",
      "100 106.31377013409768\n",
      "\n",
      "Expectation and Variance of $\\sigma(N(0,1))$:\n",
      "mu: 0.3989422804014327\n",
      "sigma squared: 0.3408450569081046\n",
      "sigma: 0.5838193701035489\n"
     ]
    }
   ],
   "source": [
    "def get_random_on_shpere(d=10):\n",
    "    x_rand = np.random.normal(size=d)\n",
    "    x_rand = x_rand / np.linalg.norm(x_rand)\n",
    "    return x_rand\n",
    "\n",
    "def get_random_normal(d=10):\n",
    "    return np.random.normal(size=d)\n",
    "\n",
    "def norm(x):\n",
    "    return np.linalg.norm(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "x = get_random_on_shpere()\n",
    "print(\"x sampled on sphere:\", x)\n",
    "print(\"x@x:\", x@x)\n",
    "print(\"norm(x):\", norm(x))\n",
    "\n",
    "print(\"\\nRelu([-1,0,1]):\", relu(np.array([-1,0,1])))\n",
    "\n",
    "print(\"\\nDiffernent values of d & x@x: (x random normal)\")\n",
    "d_vals = [2, 5, 10, 20, 50, 100]\n",
    "for d in d_vals:\n",
    "    x = get_random_normal(d=d)\n",
    "    print(d, x@x)\n",
    "\n",
    "\n",
    "print(\"\\nExpectation and Variance of $\\sigma(N(0,1))$:\")\n",
    "mu = 1/2 * np.sqrt(2/np.pi)\n",
    "print(\"mu:\", mu)\n",
    "sigma_sq = 1/2 - mu**2\n",
    "print(\"sigma squared:\", sigma_sq)\n",
    "sigma = np.sqrt(sigma_sq)\n",
    "print(\"sigma:\", sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "THis is output shape\n",
      "()\n",
      "0.12183915021157674\n",
      "THis is output shape\n",
      "()\n",
      "0.1559793126229049\n",
      "THis is output shape\n",
      "()\n",
      "0.10893972282785533\n",
      "THis is output shape\n",
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.1296555 , 0.13350936, 0.1220565 , 0.13310335, 0.09540112,\n",
       "       0.08182175, 0.10711462, 0.0914269 , 0.10252661, 0.14796253,\n",
       "       0.12901997, 0.09875946, 0.09358285, 0.12264399, 0.10768276,\n",
       "       0.13082911, 0.13533608, 0.10946237, 0.13902944, 0.12023533,\n",
       "       0.14274269, 0.12948096, 0.14819991, 0.13513668, 0.13154457,\n",
       "       0.09591074, 0.09966106, 0.13387786, 0.13548344, 0.11192053,\n",
       "       0.15148666, 0.139645  , 0.1461126 , 0.14053114, 0.09929314,\n",
       "       0.11176383, 0.12872461, 0.12623772, 0.16167379, 0.14951806,\n",
       "       0.13974087, 0.14810253, 0.15025452, 0.1247771 , 0.14437849,\n",
       "       0.11332856, 0.13722175, 0.10453206, 0.13324086, 0.12376675,\n",
       "       0.11515096, 0.12261502, 0.14191785, 0.14832083, 0.15611214,\n",
       "       0.13576719, 0.16018268, 0.14129496, 0.12303491, 0.09465069,\n",
       "       0.15177715, 0.15328349, 0.12774719, 0.11239061, 0.11061417,\n",
       "       0.11994464, 0.1280485 , 0.16124671, 0.13523021, 0.14115464,\n",
       "       0.10830828, 0.10466786, 0.12997094, 0.13936428, 0.13207755,\n",
       "       0.13262157, 0.1266326 , 0.1022847 , 0.11882634, 0.14754316,\n",
       "       0.15279843, 0.11574815, 0.12326919, 0.12965049, 0.15943608,\n",
       "       0.14090328, 0.12657417, 0.10870287, 0.15571252, 0.13639078,\n",
       "       0.14013401, 0.1437947 , 0.15293534, 0.09793936, 0.13525348,\n",
       "       0.13654477, 0.11099366, 0.140195  , 0.12834131, 0.09586534,\n",
       "       0.13929484, 0.11153833, 0.12968959, 0.10924414, 0.13106774,\n",
       "       0.14347742, 0.15202464, 0.12835427, 0.12247658, 0.09457048,\n",
       "       0.14901609, 0.10748449, 0.11040234, 0.1629044 , 0.12425243,\n",
       "       0.08530338, 0.11146197, 0.13652321, 0.12265582, 0.11309415,\n",
       "       0.12051477, 0.12240022, 0.09947785, 0.15152998, 0.17130734,\n",
       "       0.13691854, 0.13384645, 0.09954455, 0.13703151, 0.12888992,\n",
       "       0.14224241, 0.11454855, 0.12624908, 0.1029057 , 0.10327398,\n",
       "       0.12527981, 0.13464032, 0.11906233, 0.12824828, 0.10589077,\n",
       "       0.13338847, 0.10804405, 0.11439241, 0.11085851, 0.1337444 ,\n",
       "       0.15125588, 0.10506039, 0.13027226, 0.11624928, 0.14651327,\n",
       "       0.10426531, 0.15088521, 0.14200226, 0.09873221, 0.1101696 ,\n",
       "       0.11763833, 0.11048614, 0.13774183, 0.14856801, 0.15470488,\n",
       "       0.13070915, 0.14307016, 0.15130071, 0.15063072, 0.13736379,\n",
       "       0.15180133, 0.13299501, 0.13172057, 0.11133344, 0.17468919,\n",
       "       0.14367738, 0.14611982, 0.1207891 , 0.11587945, 0.11538246,\n",
       "       0.12858519, 0.12884158, 0.09867143, 0.11936173, 0.14034348,\n",
       "       0.14790514, 0.12508568, 0.14815668, 0.15711731, 0.1345698 ,\n",
       "       0.12231506, 0.13131083, 0.1191833 , 0.12126224, 0.11648005,\n",
       "       0.12449984, 0.15818889, 0.12663892, 0.16003135, 0.16115564,\n",
       "       0.10475733, 0.11225208, 0.16439538, 0.10322894, 0.10952085,\n",
       "       0.1218469 , 0.13235496, 0.16780684, 0.12107794, 0.12500443,\n",
       "       0.13015169, 0.13437718, 0.16246296, 0.1178068 , 0.12570177,\n",
       "       0.13197459, 0.12577694, 0.15255058, 0.14351943, 0.11659171,\n",
       "       0.15050157, 0.13232965, 0.10641336, 0.14090045, 0.11678185,\n",
       "       0.10194924, 0.1050205 , 0.1161804 , 0.14361044, 0.16069824,\n",
       "       0.12281573, 0.11860098, 0.15958061, 0.1338711 , 0.10779505,\n",
       "       0.09503512, 0.14583535, 0.11409943, 0.10690314, 0.1321331 ,\n",
       "       0.12682797, 0.09719018, 0.14405561, 0.15637269, 0.11017401,\n",
       "       0.12319787, 0.12191048, 0.10161594, 0.13912458, 0.15717848,\n",
       "       0.1101076 , 0.14985951, 0.13383692, 0.14969783, 0.11659189,\n",
       "       0.12733408, 0.12817355, 0.12795945, 0.12956884, 0.11181715,\n",
       "       0.13914559, 0.12928022, 0.16853396, 0.1068158 , 0.15675967,\n",
       "       0.14087999, 0.11823413, 0.11074294, 0.12760731, 0.15398234,\n",
       "       0.13497629, 0.15068933, 0.1285169 , 0.10322435, 0.13779353,\n",
       "       0.12924556, 0.10165647, 0.1066407 , 0.14012368, 0.1269559 ,\n",
       "       0.1275157 , 0.11806796, 0.11379587, 0.10218134, 0.14649178,\n",
       "       0.12923668, 0.16271296, 0.12731954, 0.14726765, 0.09691187,\n",
       "       0.10802274, 0.1150557 , 0.14471665, 0.15699036, 0.13281593,\n",
       "       0.1448665 , 0.10273835, 0.11105379, 0.14965847, 0.11955346,\n",
       "       0.14521514, 0.14179788, 0.10802057, 0.16263721, 0.13490267,\n",
       "       0.10181696, 0.15473913, 0.15094584, 0.14053891, 0.12467618,\n",
       "       0.12510411, 0.12747176, 0.12942774, 0.08853841, 0.11778009,\n",
       "       0.11941597, 0.12131752, 0.11483053, 0.11677327, 0.142925  ,\n",
       "       0.12664802, 0.13974157, 0.11443087, 0.12914303, 0.12851513,\n",
       "       0.10345247, 0.14988155, 0.17025541, 0.12856924, 0.14455664,\n",
       "       0.10938926, 0.13967596, 0.13097203, 0.13990886, 0.13206578,\n",
       "       0.12232287, 0.13260995, 0.09850584, 0.12176086, 0.12015742,\n",
       "       0.11082983, 0.14149317, 0.10521186, 0.13566643, 0.13864103,\n",
       "       0.12022771, 0.14839047, 0.11316471, 0.12166567, 0.14644965,\n",
       "       0.13667168, 0.17608522, 0.13055848, 0.10933258, 0.13163643,\n",
       "       0.14141856, 0.14235736, 0.12177086, 0.12631638, 0.1203881 ,\n",
       "       0.13274508, 0.14634556, 0.11403715, 0.13850259, 0.12413198,\n",
       "       0.12013766, 0.11042234, 0.1360636 , 0.14211446, 0.16390445,\n",
       "       0.11031029, 0.11613435, 0.13720111, 0.11297687, 0.1081113 ,\n",
       "       0.15196512, 0.10777856, 0.13556234, 0.11699042, 0.121279  ,\n",
       "       0.12516377, 0.13608401, 0.1354906 , 0.13506912, 0.13640875,\n",
       "       0.13542618, 0.13061764, 0.12711572, 0.1567589 , 0.13458108,\n",
       "       0.13024978, 0.14460589, 0.14246142, 0.13025431, 0.10117889,\n",
       "       0.11345994, 0.10378224, 0.12829623, 0.12759878, 0.10690717,\n",
       "       0.10774959, 0.12381982, 0.1145215 , 0.11257815, 0.12341484,\n",
       "       0.14392221, 0.13279715, 0.10311773, 0.11878926, 0.12727512,\n",
       "       0.13336363, 0.10216508, 0.10424183, 0.11419772, 0.15094584,\n",
       "       0.11493329, 0.13476293, 0.09887513, 0.13925083, 0.13934953,\n",
       "       0.14009573, 0.13735424, 0.1288536 , 0.14495806, 0.10553251,\n",
       "       0.12240869, 0.12442345, 0.1149331 , 0.12555918, 0.12470806,\n",
       "       0.12914366, 0.1294566 , 0.16054713, 0.11453066, 0.15180879,\n",
       "       0.13034688, 0.13106571, 0.11766325, 0.1312817 , 0.14845837,\n",
       "       0.1058757 , 0.11537682, 0.12675803, 0.12248724, 0.11681715,\n",
       "       0.11552951, 0.12737776, 0.15317411, 0.14020239, 0.13197251,\n",
       "       0.147251  , 0.17511207, 0.15746977, 0.13697043, 0.12418942,\n",
       "       0.11511651, 0.10537258, 0.15174872, 0.11654368, 0.11917961,\n",
       "       0.12346989, 0.12928145, 0.14180963, 0.12035539, 0.1243336 ,\n",
       "       0.11324979, 0.11012962, 0.1326733 , 0.15135951, 0.11283029,\n",
       "       0.1500936 , 0.12557749, 0.14247668, 0.12983352, 0.13727067,\n",
       "       0.1289849 , 0.12488705, 0.13127639, 0.12076587, 0.11487918,\n",
       "       0.15070623, 0.11837359, 0.16198165, 0.10631827, 0.10582702,\n",
       "       0.11823741, 0.12443993, 0.12700985, 0.12967941, 0.10418359,\n",
       "       0.13216908, 0.12412275, 0.11701454, 0.1368827 , 0.13247682,\n",
       "       0.1640573 , 0.14601332, 0.15006896, 0.0998239 , 0.14319994,\n",
       "       0.10800833, 0.14490172, 0.11469608, 0.13204558, 0.1454363 ,\n",
       "       0.09583435, 0.12690385, 0.11539449, 0.13928302, 0.1158262 ,\n",
       "       0.12127367, 0.13130143, 0.14118157, 0.11171402, 0.11783655,\n",
       "       0.14642846, 0.15483554, 0.16293478, 0.10954905, 0.1368475 ,\n",
       "       0.16856339, 0.10621846, 0.12284708, 0.12833959, 0.14558064,\n",
       "       0.13538227, 0.13218717, 0.1138398 , 0.11588514, 0.14689746,\n",
       "       0.12556801, 0.12787673, 0.12606649, 0.13082286, 0.14400277,\n",
       "       0.10753784, 0.13724663, 0.1288715 , 0.09401331, 0.16432816,\n",
       "       0.13826328, 0.13207063, 0.13281788, 0.1258194 , 0.15132934,\n",
       "       0.12736367, 0.13089563, 0.11779456, 0.13971953, 0.12525871,\n",
       "       0.0945932 , 0.11721974, 0.12069172, 0.11025688, 0.12996575,\n",
       "       0.10999051, 0.14216234, 0.09899205, 0.15044246, 0.15138228,\n",
       "       0.10850373, 0.10762733, 0.11250877, 0.15648396, 0.14290505,\n",
       "       0.1190545 , 0.14429769, 0.10521994, 0.12230001, 0.13333393,\n",
       "       0.11952334, 0.13512047, 0.12207964, 0.1634309 , 0.15106296,\n",
       "       0.12375407, 0.13142823, 0.10449673, 0.12201825, 0.12753469,\n",
       "       0.12187877, 0.13565762, 0.09885838, 0.13547219, 0.11420692,\n",
       "       0.15848751, 0.11270039, 0.14234501, 0.11254315, 0.09604295,\n",
       "       0.12140557, 0.1372222 , 0.1240185 , 0.11236356, 0.11501026,\n",
       "       0.12909155, 0.11993339, 0.12676993, 0.15722926, 0.10462818,\n",
       "       0.1333098 , 0.08586367, 0.12391665, 0.12599376, 0.09660477,\n",
       "       0.15564594, 0.14821444, 0.11611444, 0.09752486, 0.10609426,\n",
       "       0.10902007, 0.14638549, 0.1551908 , 0.12827965, 0.13697544,\n",
       "       0.10391203, 0.13038843, 0.15101611, 0.14099009, 0.10920809,\n",
       "       0.15757388, 0.129818  , 0.13927871, 0.14144399, 0.15301889,\n",
       "       0.10584992, 0.0968676 , 0.12276088, 0.1323065 , 0.11484042,\n",
       "       0.1538769 , 0.15839099, 0.15504467, 0.10828188, 0.12194625,\n",
       "       0.16817441, 0.0817002 , 0.11807059, 0.12562384, 0.160188  ,\n",
       "       0.11462209, 0.14101774, 0.10732617, 0.12073921, 0.13520354,\n",
       "       0.11399541, 0.1225006 , 0.15592931, 0.12432746, 0.13244336,\n",
       "       0.14228604, 0.11525576, 0.10173882, 0.16135757, 0.1132676 ,\n",
       "       0.13705282, 0.10536075, 0.13701678, 0.12897473, 0.15832349,\n",
       "       0.13423748, 0.09305114, 0.12347995, 0.14480307, 0.10534615,\n",
       "       0.15772726, 0.14102025, 0.15227916, 0.14969448, 0.11293709,\n",
       "       0.10501609, 0.12046289, 0.10184842, 0.13607806, 0.13720637,\n",
       "       0.10089204, 0.15704114, 0.10562135, 0.14812531, 0.10550667,\n",
       "       0.12250338, 0.10687457, 0.11565935, 0.12919397, 0.1105324 ,\n",
       "       0.12887628, 0.10650187, 0.15168518, 0.12748451, 0.14077233,\n",
       "       0.13553822, 0.12077742, 0.1055848 , 0.11951907, 0.15181965,\n",
       "       0.11648193, 0.12586942, 0.13017884, 0.12136842, 0.10065001,\n",
       "       0.13611965, 0.12179069, 0.16267899, 0.13803032, 0.13287043,\n",
       "       0.12160326, 0.10441336, 0.12898183, 0.1420241 , 0.14829919,\n",
       "       0.14971328, 0.1305501 , 0.15351095, 0.12699846, 0.16003571,\n",
       "       0.13351344, 0.10147041, 0.15093891, 0.14712343, 0.15599956,\n",
       "       0.16002379, 0.16784057, 0.15841467, 0.16173616, 0.16708891,\n",
       "       0.14437255, 0.12859611, 0.12127887, 0.13943357, 0.11016161,\n",
       "       0.113862  , 0.12048286, 0.12970497, 0.12029854, 0.0978931 ,\n",
       "       0.13523643, 0.11502925, 0.11026038, 0.11490501, 0.13073098,\n",
       "       0.10222682, 0.11965483, 0.1156953 , 0.16362076, 0.1328341 ,\n",
       "       0.09454168, 0.10504893, 0.13288477, 0.10005244, 0.13452768,\n",
       "       0.14821437, 0.11536179, 0.10220494, 0.13601653, 0.12610028,\n",
       "       0.14813743, 0.09519653, 0.09600918, 0.09991205, 0.12208769,\n",
       "       0.09878931, 0.13236068, 0.11777439, 0.12428564, 0.11979145,\n",
       "       0.16538091, 0.16756509, 0.13243582, 0.14650314, 0.17567273,\n",
       "       0.14458315, 0.12424818, 0.12885209, 0.14683642, 0.12882938,\n",
       "       0.13015061, 0.14740819, 0.09532421, 0.15091341, 0.11943124,\n",
       "       0.14301987, 0.10090317, 0.13333325, 0.15696401, 0.13587905,\n",
       "       0.16124395, 0.09730025, 0.13134713, 0.13610258, 0.1431342 ,\n",
       "       0.16152866, 0.13801834, 0.1074931 , 0.11825965, 0.15253349,\n",
       "       0.16225239, 0.14028158, 0.14375497, 0.15921499, 0.12069175,\n",
       "       0.12197836, 0.1170371 , 0.10928351, 0.11379972, 0.12175796,\n",
       "       0.15733998, 0.14312508, 0.12895635, 0.12220363, 0.10157314,\n",
       "       0.13964861, 0.14603906, 0.15321891, 0.12984913, 0.15791303,\n",
       "       0.14439385, 0.09053222, 0.1196245 , 0.12623122, 0.14733586,\n",
       "       0.13103306, 0.12141463, 0.1331382 , 0.12736054, 0.12331517,\n",
       "       0.13978446, 0.09764479, 0.16308218, 0.14876506, 0.09620731,\n",
       "       0.14677387, 0.13036906, 0.13416775, 0.12801682, 0.12352556,\n",
       "       0.10938398, 0.16844665, 0.11967066, 0.1280643 , 0.12266524,\n",
       "       0.13837648, 0.10567507, 0.11549674, 0.11464012, 0.10646975,\n",
       "       0.15126084, 0.12371148, 0.15363654, 0.12759324, 0.09882434,\n",
       "       0.11448539, 0.11554142, 0.11195936, 0.11374366, 0.14540233,\n",
       "       0.1464415 , 0.11918511, 0.13728463, 0.12111296, 0.12183752,\n",
       "       0.10594355, 0.11547925, 0.09354623, 0.13748025, 0.10832979,\n",
       "       0.11246335, 0.1475217 , 0.11009201, 0.12829033, 0.14697719,\n",
       "       0.13294668, 0.10757112, 0.13637928, 0.11434847, 0.13212295,\n",
       "       0.13210146, 0.11561972, 0.13234258, 0.14232798, 0.11909824,\n",
       "       0.15621592, 0.12888913, 0.15817935, 0.11409087, 0.13847224,\n",
       "       0.1605579 , 0.08020399, 0.15105946, 0.14231837, 0.12753109,\n",
       "       0.12568598, 0.15189106, 0.13983329, 0.1112911 , 0.11869927,\n",
       "       0.14580912, 0.13824473, 0.11105289, 0.10792176, 0.12716502,\n",
       "       0.10593173, 0.09781918, 0.15881412, 0.13145985, 0.10377527,\n",
       "       0.1123718 , 0.12414788, 0.12006171, 0.12947806, 0.15304046,\n",
       "       0.11332171, 0.10383105, 0.15124293, 0.09576091, 0.14168634,\n",
       "       0.12094885, 0.11057019, 0.11394926, 0.1305279 , 0.12608511,\n",
       "       0.13595097, 0.11711973, 0.13089455, 0.12341894, 0.13368201,\n",
       "       0.15346803, 0.13355847, 0.14105134, 0.12919636, 0.15322851,\n",
       "       0.13438807, 0.1283813 , 0.15827772, 0.10257112, 0.11732168,\n",
       "       0.15608851, 0.15206237, 0.13002491, 0.13901802, 0.15930277,\n",
       "       0.1211191 , 0.11633189, 0.12557148, 0.1446776 , 0.12649514,\n",
       "       0.13002081, 0.12561674, 0.15451026, 0.13588308, 0.09252336,\n",
       "       0.13422696, 0.15224964, 0.09920753, 0.12679531, 0.11053948,\n",
       "       0.12014417, 0.1134416 , 0.10988128, 0.13106273, 0.11919975,\n",
       "       0.14004915, 0.10172903, 0.1392688 , 0.14651607, 0.13419113,\n",
       "       0.09887385, 0.12525702, 0.13288192, 0.12977198, 0.11047717,\n",
       "       0.13018408, 0.12201552, 0.11208562, 0.1246833 , 0.13131472,\n",
       "       0.11675319, 0.1281221 , 0.14631815, 0.10396155, 0.12692162,\n",
       "       0.11675362, 0.13669039, 0.11815266, 0.14393393, 0.16069523,\n",
       "       0.1544627 , 0.12881476, 0.13910268, 0.09758059, 0.12823419,\n",
       "       0.09704791, 0.15619794, 0.1268579 , 0.15040087, 0.13053992,\n",
       "       0.12788534, 0.16514912, 0.13871264, 0.11004834, 0.13471606,\n",
       "       0.12060985, 0.14756454, 0.13050342, 0.11519392, 0.11410476])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class network:\n",
    "    def __init__(self, N, d, on_sphere=True, nrof_repeats=1):\n",
    "        \"\"\"\n",
    "        nrof_repeats .. Number of time each weight is repeated.\n",
    "        \"\"\"\n",
    "        self.N = N # Range of weight value\n",
    "        self.d = d # Weight dimension, i.e. amount of weight scalars\n",
    "        self.on_sphere =  on_sphere\n",
    "        if on_sphere:\n",
    "            self.ws = np.array([get_random_on_shpere(d=d) for _ in range(self.N)])\n",
    "        else:\n",
    "            # self.ws = np.array([get_random_on_shpere(d=d) for _ in range(self.N)])\n",
    "            ws = []\n",
    "            for _ in range(self.N//nrof_repeats):\n",
    "                w = get_random_normal(d=d)\n",
    "                for _ in range(nrof_repeats):\n",
    "                    ws.append(w)\n",
    "            self.ws = np.array(ws)\n",
    "        print(self.ws.shape)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        ip = x@self.ws.T\n",
    "        ip_relu = relu(ip)\n",
    "        output = np.mean(ip_relu, axis=-1)\n",
    "        print(\"THis is output shape\")\n",
    "        print(output.shape)\n",
    "        return output\n",
    "        \n",
    "    def approximate_network(self, nw, rescale=True):\n",
    "        \"\"\"\n",
    "        Replaces self.ws by new weights in order to approximate the nw network.\n",
    "        This is done by sampling weights of the (old) network nw as centers,\n",
    "        and then assigning all old weights to the closest center.\n",
    "        Then the new weight (corresponding each center) \n",
    "        is the sums of the old weights assigned to this center,\n",
    "        potentially rescaled by some factor.\n",
    "        \"\"\"\n",
    "        M = self.N\n",
    "        \n",
    "        # Sample centers:\n",
    "        w_center_idcs = np.random.choice(nw.N, replace=False, size=self.N)# Chose from nw with uniform dist with N samples\n",
    "        w_centers = np.array([w/norm(w) for w in nw.ws[w_center_idcs]])\n",
    "        \n",
    "        # Assign weights to centers:\n",
    "        w_lists = [[] for _ in range(M)]#M = N(range of weights)\n",
    "        for w_old in nw.ws:\n",
    "            ips = w_centers@(w_old/norm(w_old))\n",
    "            c_ind = np.argmax(ips)\n",
    "            w_lists[c_ind].append(w_old)\n",
    "        \n",
    "        # Calculate new weights\n",
    "        new_weights = []\n",
    "        for wl in w_lists:\n",
    "            if len(wl)==0:\n",
    "                new_weights.append(np.array([0. for _ in range(self.d)]))\n",
    "                continue\n",
    "            ws_old = np.array(wl)\n",
    "            ws_sum = np.sum(ws_old, axis=0)\n",
    "            nrof_ws = ws_old.shape[0]\n",
    "            \n",
    "            if rescale:\n",
    "                if self.on_sphere:\n",
    "                    target_norm = M/nw.N * nrof_ws\n",
    "                    new_weights.append(ws_sum/np.linalg.norm(ws_sum)*target_norm)\n",
    "                else:\n",
    "                    wnorm_sum = np.sum([norm(w) for w in ws_old], axis=0)\n",
    "                    target_norm = M/nw.N * wnorm_sum\n",
    "                    new_weights.append(ws_sum/np.linalg.norm(ws_sum)*target_norm)\n",
    "            else:\n",
    "                new_weights.append(M/nw.N*ws_sum)\n",
    "                            \n",
    "        self.ws = np.array(new_weights)\n",
    "        return self\n",
    "       \n",
    "    def subsample_network(self, nw):\n",
    "        w_center_idcs = np.random.choice(nw.N, replace=False, size=self.N)\n",
    "        self.ws = np.array(nw.ws[w_center_idcs])\n",
    "        return self\n",
    "    \n",
    "    def with_opposite_weights(self):\n",
    "        new_weights = []\n",
    "        for w in self.ws:\n",
    "            new_weights.append(w)\n",
    "            new_weights.append(-w)\n",
    "            if len(new_weights)==self.N:\n",
    "                break\n",
    "        self.ws = np.array(new_weights)\n",
    "        return self\n",
    "    \n",
    "    def merge_network_weights(self, nw=None, rescale=True, keep_norm=True):\n",
    "        \n",
    "        if nw is None:\n",
    "            nw = self\n",
    "        \n",
    "        # Merge two closest weights:\n",
    "        # print(\"Merging weights from network with weight matrix shape\", nw.ws.shape)\n",
    "        nrof_weights = len(nw.ws)\n",
    "        rescaled_old_weights = nw.ws / norm(nw.ws)\n",
    "        ip_mat = rescaled_old_weights@rescaled_old_weights.T\n",
    "        np.fill_diagonal(ip_mat, -2)\n",
    "        ind_enc = np.argmax(ip_mat)\n",
    "        i, j = ind_enc//nrof_weights, ind_enc%nrof_weights\n",
    "        new_weight = nw.ws[i] + nw.ws[j]\n",
    "        \n",
    "        if keep_norm:\n",
    "            wnorm_sum = norm(nw.ws[i]) + norm(nw.ws[j])\n",
    "            assert norm(new_weight) > 0.1, \"New weight is rather close to 0, with a norm of \" + str(norm(new_weight))\n",
    "            new_weight = new_weight/norm(new_weight) * wnorm_sum\n",
    "        \n",
    "        new_weights = [new_weight]\n",
    "        for w_ind in range(len(nw.ws)):\n",
    "            if w_ind != i and w_ind != j:\n",
    "                new_weights.append(nw.ws[w_ind])\n",
    "        self.ws = np.array(new_weights)\n",
    "        \n",
    "        # Repeat if too many weights:\n",
    "        if len(self.ws) > self.N:\n",
    "            self.merge_network_weights(self, rescale=False, keep_norm=keep_norm)\n",
    "        \n",
    "        if rescale:\n",
    "            # self.ws = self.ws * (M/N)\n",
    "            self.ws = self.ws * (len(self.ws) / nrof_weights)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def greedy_forward_selection(self, nw, X, sel_subset=None):\n",
    "        if sel_subset is None:\n",
    "            sel_subset = []\n",
    "            self.old_weights = nw.ws\n",
    "            self.old_labels = nw(X)\n",
    "            current_target = self.old_labels\n",
    "        else:\n",
    "            self.ws = self.old_weights[sel_subset]\n",
    "        \n",
    "            # find best weight and add:\n",
    "            current_output = self(X)\n",
    "            nrof_weights = len(self.ws)\n",
    "            current_target = self.old_labels*(nrof_weights+1) - current_output*nrof_weights\n",
    "\n",
    "        ip = X@self.old_weights.T\n",
    "        ip_relu = relu(ip)\n",
    "        loss = (current_target[:, None] - ip_relu)**2\n",
    "        loss_per_weight = np.mean(loss, axis=0)\n",
    "        best_weight_idx = np.argmax(-loss_per_weight)\n",
    "        sel_subset.append(best_weight_idx)\n",
    "        self.ws = self.old_weights[sel_subset]\n",
    "        \n",
    "        if len(sel_subset)<self.N:\n",
    "            self.greedy_forward_selection(nw, X, sel_subset=sel_subset)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "y = network(N=100, d=10)\n",
    "print(y(get_random_on_shpere(d=10)))\n",
    "print(y(get_random_on_shpere(d=10)))\n",
    "print(y(get_random_on_shpere(d=10)))\n",
    "X = np.array([get_random_on_shpere(d=10) for _ in range(1000)])\n",
    "y(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare error for different models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(M_values, d_values, ds_size=1000, fixed_N=False, N=None, \n",
    "                    use_square_error=False, nrof_repeats=1, outname=None):\n",
    "    \n",
    "    if fixed_N:# What do we mean by fixed_N\n",
    "        assert N is not None, \"Need to provide a value for N!\"\n",
    "        print(\"Running experiments with fixed N={}.\\n\\n\".format(N))\n",
    "        M_values = M_values[M_values<=N]\n",
    "        N_values = np.array([N for _ in M_values])\n",
    "    else:\n",
    "        print(\"Running experiments with N=2M for different values of M.\\n\\n\")\n",
    "        N_values = np.array([2*M for M in M_values])\n",
    "\n",
    "    for d in d_values:\n",
    "        # For every dimension in the dimension array(which itself is an array of power of 2s)\n",
    "        X = np.array([get_random_on_shpere(d=d) for _ in range(ds_size)])\n",
    "        print(\"For d={} we get an X of \".format(d))\n",
    "        print(X)\n",
    "        return\n",
    "        for on_sphere in [True, False]:\n",
    "\n",
    "            if on_sphere:\n",
    "                continue\n",
    "                print(\"d={}, original weights sampled on hypersphere.\".format(d))\n",
    "            else:\n",
    "                print(\"d={}, original weights sampled from Gaussian.\".format(d))\n",
    "                print\n",
    "\n",
    "            errors_dict = {}\n",
    "\n",
    "            for M in M_values:\n",
    "                \n",
    "                if not fixed_N:\n",
    "                    N = 2*M\n",
    "\n",
    "                if M>N:\n",
    "                    continue\n",
    "\n",
    "                yhats_dict = {}\n",
    "\n",
    "                y_nw = network(N=N, d=d, on_sphere=on_sphere, nrof_repeats=nrof_repeats)\n",
    "                y = y_nw(X)\n",
    "\n",
    "                mu = 1/2 * np.sqrt(2/np.pi)\n",
    "                yhats_dict[\"constant\"] = np.array([mu for _ in X])\n",
    "\n",
    "                yhat_nw = network(N=M, d=d, on_sphere=on_sphere)\n",
    "                yhats_dict[\"random weights\"] = yhat_nw(X)\n",
    "\n",
    "                yhat_nw = network(N=M, d=d, on_sphere=on_sphere).with_opposite_weights()\n",
    "                yhats_dict[\"random opposites weights\"] = yhat_nw(X)\n",
    "\n",
    "                yhat_nw = network(N=M, d=d, on_sphere=on_sphere).subsample_network(y_nw)\n",
    "                yhats_dict[\"subsample\"] = yhat_nw(X)\n",
    "\n",
    "                yhat_nw = network(N=M, d=d, on_sphere=on_sphere).approximate_network(y_nw, rescale=False)\n",
    "                yhats_dict[\"sum weights locally\"] = yhat_nw(X)\n",
    "\n",
    "                yhat_nw = network(N=M, d=d, on_sphere=on_sphere).approximate_network(y_nw)\n",
    "                yhats_dict[\"sum weights locally + rescale\"] = yhat_nw(X)\n",
    "\n",
    "                yhat_nw = network(N=M, d=d, on_sphere=on_sphere).greedy_forward_selection(y_nw, X)\n",
    "                yhats_dict[\"forward selection\"] = yhat_nw(X)\n",
    "\n",
    "                # yhat_nw_merge = network(N=M, d=d, on_sphere=on_sphere).merge_network_weights(y_nw, keep_norm=False)\n",
    "                # # yhat_nw_merge = network(N=M, d=d, on_sphere=on_sphere).merge_network_weights(y_nw, keep_norm=True)\n",
    "                # yhats_dict[\"merge\"] = yhat_nw_merge(X)\n",
    "                # yhats_dict[\"merge + correction\"] = yhat_nw_merge(X)*((N/M)**(1/2))\n",
    "\n",
    "                for key, yhat in yhats_dict.items():\n",
    "                    if use_square_error:\n",
    "                        error = np.mean((y-yhat)**2)\n",
    "                    else:\n",
    "                        error = np.mean(np.abs(y-yhat))\n",
    "                    if key not in errors_dict.keys():\n",
    "                        errors_dict[key] = []\n",
    "                    errors_dict[key].append(error)\n",
    "\n",
    "            plt.figure(figsize=(12,6))\n",
    "\n",
    "            color_dict = {# \"constant\": \"silver\",\n",
    "                          \"random weights\": \"blue\",\n",
    "                          # \"random opposites weights\": \"darkblue\",\n",
    "                          \"subsample\": \"purple\",\n",
    "                          \"sum weights locally\": \"orange\",\n",
    "                          \"sum weights locally + rescale\": \"red\",\n",
    "                          # \"merge\": \"red\",\n",
    "                          # \"merge + correction\": \"brown\",\n",
    "                          \"forward selection\": \"aqua\"}\n",
    "            for key, errors in errors_dict.items():\n",
    "                if key not in color_dict.keys():\n",
    "                    continue\n",
    "                plt.plot(M_values, errors, label=key, color=color_dict.get(key, \"black\"))\n",
    "\n",
    "            \n",
    "            exp = 1. if use_square_error else 1/2\n",
    "            label_str = \"${}$\" if use_square_error else \"$\\sqrt{{{}}}$\"\n",
    "            \n",
    "            start_value = sigma**2\n",
    "            plt.plot(M_values, (start_value*(M_values**(-1.) + N_values**(-1.)))**exp, \n",
    "                     '--', color=\"blue\", alpha=0.4, label=label_str.format(\"\\sigma^2 (1/M + 1/N)\"))\n",
    "                \n",
    "            start_value = sigma**2\n",
    "            plt.plot(M_values, (start_value*(M_values**(-1.) - N_values**(-1.)))**exp, \n",
    "                     '--', color=\"purple\", alpha=0.4, label=label_str.format(\"\\sigma^2 (1/M - 1/N)\"))\n",
    "\n",
    "            start_value = sigma**2\n",
    "            plt.plot(M_values, (start_value*M_values**(-2.))**exp, \n",
    "                     '--', color=\"aqua\", alpha=0.4, label=label_str.format(\"\\sigma^2 / M^2\"))\n",
    "                \n",
    "            scaling_exp = -4/(d-1)\n",
    "            start_value = sigma**2\n",
    "            plt.plot(M_values, (start_value*M_values**scaling_exp)**exp, \n",
    "                     '--', color=\"orange\", alpha=0.4, \n",
    "                     label=label_str.format(\"\\sigma^2 / M^{{4/(d-1)}}\".format(scaling_exp)))\n",
    "            \n",
    "            \n",
    "            plt.xscale(\"log\")\n",
    "            plt.xticks(M_values)\n",
    "            plt.gca().xaxis.set_major_formatter(plt_ticker.ScalarFormatter())\n",
    "            plt.yscale(\"log\")\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "            plt.xlabel(\"M\")\n",
    "            if use_square_error:\n",
    "                plt.ylabel(\"Squared Error\")\n",
    "                err_tit = \"MSE\"\n",
    "                outerr = \"mse\"\n",
    "            else:\n",
    "                plt.ylabel(\"Absolute Error\")\n",
    "                err_tit = \"Abs. Err.\"\n",
    "                outerr = \"abserr\"\n",
    "            if fixed_N:\n",
    "                plt.title(\"{} for N={} and d={}\".format(err_tit, N, d))\n",
    "            else:\n",
    "                plt.title(\"{} for N=2M and d={}\".format(err_tit, d))\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if outname is not None:\n",
    "                plt.savefig(os.path.join(\"{}\",\"{}_{}_d{}.png\").format(\n",
    "                    outfolder, outname, outerr, d))\n",
    "            plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with N=2M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments with N=2M for different values of M.\n",
      "\n",
      "\n",
      "For d=2 we get an X of \n",
      "[[ 0.97094261 -0.23931246]\n",
      " [-0.27848299  0.96044116]\n",
      " [ 0.39097683  0.92040052]\n",
      " ...\n",
      " [-0.3858556  -0.92255919]\n",
      " [-0.71110636 -0.70308445]\n",
      " [-0.94595276 -0.32430445]]\n"
     ]
    }
   ],
   "source": [
    "M_values = np.array([2**i for i in range(10)]) # Powers of 2 in the range of 10\n",
    "d_values = np.array([2**i for i in range(1, 9)]) # PMore ore less the same, trimming the end values of M_values\n",
    "ds_size = 1000\n",
    "fixed_N = False\n",
    "    \n",
    "run_experiments(M_values, d_values, fixed_N=fixed_N, use_square_error=True, outname=\"Neqals2M\")\n",
    "# run_experiments(M_values, d_values, fixed_N=fixed_N, use_square_error=False, outname=\"Neqals2M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with fixed N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments with fixed N=2000.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M_values = np.array([2**i for i in range(10)]) \n",
    "d_values = np.array([2**i for i in range(1, 9)])\n",
    "ds_size = 1000\n",
    "fixed_N = True\n",
    "\n",
    "N = 2000\n",
    "\n",
    "run_experiments(M_values, d_values, fixed_N=True, N=N, use_square_error=True, outname=\"fixedN\")\n",
    "# run_experiments(M_values, d_values, fixed_N=True, N=N, use_square_error=False, outname=\"fixedN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Variance of ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing simulation with 1 different ws.\n",
      "0.34458253414561135  -  [0.4106566918794706, 0.23131574364315496, 0.2628452382991855, 0.4633431165291635, 0.35977543328267725, 0.2944165753461859, 0.3944971197296501, 0.3595041909174097]\n",
      "\n",
      "Doing simulation with 2 different ws.\n",
      "0.10262827413855714  -  [0.14434437506311168, 0.09092075769869044, 0.11241073985809989, 0.09393828003580759, 0.09153402035057415, 0.10551709140203763, 0.12737846067459987, 0.09909732584819125]\n",
      "\n",
      "Doing simulation with 4 different ws.\n",
      "0.03935595210612639  -  [0.025085735735734044, 0.04407142065546617, 0.043873341932701745, 0.05164524989892451, 0.04590756971389212, 0.0313944837995218, 0.032569908253090474, 0.041289981881968224]\n",
      "\n",
      "Doing simulation with 8 different ws.\n",
      "0.018103446881913905  -  [0.023672750763997642, 0.02090455007545466, 0.016708646865256634, 0.01668336141577037, 0.015068441805361297, 0.015110704843930397, 0.016833991422964437, 0.015916405362227336]\n",
      "\n",
      "Doing simulation with 16 different ws.\n",
      "0.009027339461740754  -  [0.007295056698314664, 0.010450390978863619, 0.008131334890755453, 0.008834486634788863, 0.0099075474147402, 0.007995584052014844, 0.00893843372320841, 0.009335523455697665]\n",
      "\n",
      "Doing simulation with 32 different ws.\n",
      "0.004941514388126427  -  [0.0050930077730158745, 0.005000537726232887, 0.0037414836853645935, 0.005228195819606863, 0.00567603996622562, 0.00508864481440621, 0.004310344910154113, 0.005117391699339797]\n",
      "\n",
      "Doing simulation with 64 different ws.\n",
      "0.0030929620635490535  -  [0.003460210306773815, 0.0026481145277323817, 0.0026275167078273235, 0.0035312425147064064, 0.0038120571221947003, 0.0036202836602465226, 0.002296345429564832, 0.0030765516914014472]\n",
      "\n",
      "Doing simulation with 128 different ws.\n",
      "0.0021509180164335064  -  [0.0017176149061936367, 0.0022697308548176656, 0.0016896050200421306, 0.0023302093271100255, 0.001945240724587569, 0.0028478868519644226, 0.002428181148119051, 0.002069307441532837]\n",
      "\n",
      "Doing simulation with 256 different ws.\n",
      "0.0017088408321588983  -  [0.0013962105571087723, 0.0018292630620205996, 0.0019411738191470536, 0.0014869502914356617, 0.0013199519542069302, 0.002065377516807464, 0.001417069241078229, 0.0017172021349475147]\n",
      "\n",
      "Doing simulation with 512 different ws.\n",
      "0.0015009153412579189  -  [0.0012384026074486112, 0.001721789594428196, 0.0014734846739226821, 0.0013571911493251891, 0.001819385187764522, 0.001460264183581688, 0.0010792616831033668, 0.0010910110975761063]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   4   8  16  32  64 128 256 512]\n",
      "[0.34458253414561135, 0.10262827413855714, 0.03935595210612639, 0.018103446881913905, 0.009027339461740754, 0.004941514388126427, 0.0030929620635490535, 0.0021509180164335064, 0.0017088408321588983, 0.0015009153412579189]\n",
      "[0.34458253414561135, 0.2052565482771143, 0.15742380842450557, 0.14482757505531124, 0.14443743138785206, 0.15812846042004566, 0.19794957206713942, 0.2753175061034888, 0.43746325303267797, 0.7684686547240545]\n"
     ]
    }
   ],
   "source": [
    "d = 64\n",
    "# d_range = [1]\n",
    "# nrof_ws_range = [1, 2, 3, 5, 10, 25]\n",
    "# nrof_ws_range = [1, 2, 3, 5, 10, 25, 50, 100]\n",
    "nrof_ws_range = np.array([2**i for i in range(10)])\n",
    "\n",
    "nrof_experiments = 100\n",
    "nrof_x_samples = 100\n",
    "\n",
    "exp_var_list = []\n",
    "\n",
    "avg_corrs_list = []\n",
    "\n",
    "# for d in d_range:\n",
    "for nrof_ws in nrof_ws_range:\n",
    "    # print(\"Doing simulation with d={}:\".format(d))\n",
    "    print(\"\\nDoing simulation with {} different ws.\".format(nrof_ws))\n",
    "    \n",
    "    \"\"\"\n",
    "    Array in each experiments corrensponds to:\n",
    "    [diffenrent inits of ws, different inits of X, different ws, ..]\n",
    "    \"\"\"\n",
    "    \n",
    "    avg_corrs = []\n",
    "    for _ in range(nrof_experiments):\n",
    "        \n",
    "        # Sample weights:\n",
    "        # ws = np.array([get_random_normal(d=d) for _ in range(nrof_ws)])\n",
    "        ws = np.array([get_random_on_shpere(d=d) for _ in range(nrof_ws)])\n",
    "        \n",
    "        # Adverserial weights:\n",
    "        cumm_weight_sum = np.array([0 for _ in range(d)])\n",
    "        ws_list = []\n",
    "        for _ in range(nrof_ws):\n",
    "            w_basis = get_random_on_shpere(d=d)\n",
    "            w_dir = w_basis - cumm_weight_sum\n",
    "            w = w_dir / np.linalg.norm(w_dir)\n",
    "            cumm_weight_sum = cumm_weight_sum + w\n",
    "            ws_list.append(w)\n",
    "        ws = np.array(ws_list)\n",
    "        \n",
    "        # print(ws.shape)\n",
    "        X = np.array([get_random_normal(d=d) for _ in range(nrof_x_samples)])\n",
    "        # print(X.shape)\n",
    "        ip = X@ws.T\n",
    "        ip_relu = relu(ip)\n",
    "        # print(ip_relu.shape)\n",
    "        \n",
    "        sample_mean = np.mean(ip_relu, axis=0)\n",
    "        # print(\"sample_mean\", sample_mean)\n",
    "        \n",
    "        sample_var = np.mean(ip_relu*ip_relu, axis=0) - sample_mean**2\n",
    "        # print(\"sample_var\", sample_var)\n",
    "        # Analytic variance:\n",
    "        an_exp_xsquared = 1/2 * (d*1)\n",
    "        # print(\"theoretical var?\", an_exp_xsquared - sample_mean**2)\n",
    "        \n",
    "        # corr_mat = np.mean(ip_relu[:, None, :]*ip_relu[:, :, None], axis=0) - sample_mean**2\n",
    "        corr_mat = np.mean(ip_relu[:, None, :]*ip_relu[:, :, None], axis=0) - sample_mean[:, None]*sample_mean[None, :]\n",
    "        # print(\"corr_mat\", corr_mat)\n",
    "        \n",
    "        avg_corr = np.mean(corr_mat)\n",
    "        # print(\"avg_corr\", avg_corr)\n",
    "        \n",
    "        \n",
    "        var_mat = np.diag(np.diag(corr_mat))\n",
    "        avg_var = np.mean(var_mat)\n",
    "        # avg_corr = avg_var\n",
    "        avg_corrs.append(avg_corr)\n",
    "    \n",
    "        # break\n",
    "        \n",
    "    print(np.mean(avg_corrs), \" - \", avg_corrs[:8])\n",
    "    avg_corrs_list.append(np.mean(avg_corrs))\n",
    "    # break\n",
    "\n",
    "avg_corrs_list_rescales = [ac*w for ac, w in zip(nrof_ws_range, avg_corrs_list)]\n",
    "plt.plot(nrof_ws_range, avg_corrs_list, label=\"variance\")\n",
    "plt.plot(nrof_ws_range, avg_corrs_list_rescales, label=\"var*M\")\n",
    "plt.title(\"Variance for d={}\".format(d))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(nrof_ws_range)\n",
    "print(avg_corrs_list)\n",
    "print(avg_corrs_list_rescales)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set off-diagonal elements of the corr matrix to zero,\n",
    "then the mean (of the variances) behaves like 1/M.<br>\n",
    "But if we consider all elements of the correlation matrix,\n",
    "and pick the weights randomly, \n",
    "it seems like the c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
